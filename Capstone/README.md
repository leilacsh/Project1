# Background
In the world of finance, timely and accurate information can be the difference between success and failure. Financial news and headlines can have a significant impact on market movements, and traders, investors, and analysts rely heavily on such news to make informed decisions. However, manually analyzing and classifying the sentiment of financial news can be a time-consuming and challenging task, especially given the vast amount of data that is available.

To overcome this challenge, there has been a growing interest in using automated sentiment analysis systems that can quickly and accurately analyze and classify the sentiment of financial news to predict market movements. Sentiment analysis involves extracting subjective information from text data, such as positive or negative opinions, emotions, and attitudes. By analyzing the sentiment of financial news, traders and investors can gain insights into market trends and make better investment decisions.

In recent years, machine learning models such as BERT (Bidirectional Encoder Representations from Transformers) and FinBERT have emerged as powerful tools for natural language processing tasks, including sentiment analysis. BERT is a pre-trained language model that uses a transformer-based architecture and has achieved state-of-the-art performance on various natural language processing tasks. FinBERT is a specialized version of BERT that is specifically trained on financial news and documents.

Given the potential of these models to improve the accuracy and efficiency of sentiment analysis for financial headlines, this project aims to develop a sentiment analysis system for financial news using BERT and FinBERT. By training and fine-tuning these models on a large dataset of financial headlines, we hope to develop a system that can accurately predict the sentiment of financial news, particularly for stock markets. Such a system could assist traders, investors, and analysts in making informed decisions and provide valuable insights into market trends.

# Problem Statement
The financial industry generates a vast amount of textual data, such as news articles, press releases, and earnings reports. Extracting sentiment from this data is crucial for making informed investment decisions. However, traditional sentiment analysis models often struggle to accurately capture the nuances and complexities of financial language. To overcome this challenge, I propose using word embedding models, BERT, and FinBERT, to improve the accuracy of sentiment analysis on financial news.

The goal is to compare the performance of these three models in capturing the sentiment of financial news articles. I will fine-tune the pretrained BERT and FinBERT models on the Kaggle dataset and evaluate their performance on the sentiment classification task. I will also compare the performance of these models to a traditional sentiment analysis model that uses word embeddings generated by Keras. By comparing the performance of these models, I aim to provide insights into the best approach for sentiment analysis on financial news data.

In short, the objective of this project is to perform sentiment analysis on financial news to classify the sentiment of the news as positive, negative or neutral. The metric used are accuracy and f1 score.

# Dataset
Dataset is retrieved from Kaggle notebook created by Ankur Sinha. Refer https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news.

This dataset (FinancialPhraseBank) contains the sentiments for financial news headlines from the perspective of a retail investor.

# Best Result from each Model

||Training Accuracy|Validation Accuracy|Testing Accuracy|Testing Weighted Average F1 Score|
|---|---|---|---|---|
|**Best Word Emb Model 1.8**|0.7867|0.7503|0.77|0.77|
|**Best BERT Model 2**|0.9635|0.8669|0.84|0.84|
|**Best FinBERT Model 1.1**|0.7898|0.8431|0.85|0.84|

# Conclusion
Word embedding is a technique that represents words as numerical vectors, which can be used to capture the semantic relationships between words. However, it does not capture the context in which words are used, which leads to inaccurate sentiment analysis.

BERT, on the other hand, is a pre-trained language model that uses a bidirectional transformer architecture to capture the context in which words are used. However, BERT is computationally expensive to fine-tune the model.

FinBERT is a pre-trained language model that has been specifically designed for financial sentiment analysis. It is based on the BERT architecture and has been fine-tuned on financial news articles and social media posts.

In this project, FinBERT has been shown to achieve higher accuracy in financial sentiment analysis than the general-purpose language model BERT, with reported accuracy rates of up to 85% and weighted average F1 score of up to 84%.

# Deployment
The best FinBERT model has been implemented on AWS through Flask.



