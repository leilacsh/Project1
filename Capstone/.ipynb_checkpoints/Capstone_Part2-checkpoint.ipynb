{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f315530",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project:  Sentiment Analysis on Financial News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863df45a",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db8a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb51509",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2cba9",
   "metadata": {},
   "source": [
    "##### Function to scrap latest 100 headlines from Finviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e864d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finviz_headlines(ticker, start_date=None, end_date=None):\n",
    "        \n",
    "    # create an empty dataframe\n",
    "    df_concat = pd.DataFrame()\n",
    "    \n",
    "    for t in ticker:\n",
    "        \n",
    "        path = r\"C:\\ChromeDriver\\chromedriver.exe\"\n",
    "        service = Service(path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get('https://finviz.com')\n",
    "        \n",
    "        search = driver.find_element(By.XPATH, \"//input[@placeholder='Search ticker, company or profile']\")\n",
    "        search.send_keys(t)\n",
    "        search.send_keys(Keys.RETURN)  \n",
    "\n",
    "        try:\n",
    "            # Wait up to 10 seconds for the title element to be present\n",
    "            WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//title\")))     \n",
    "\n",
    "            # Check that the title contains the text \"GOOG\"\n",
    "            if t in driver.title:\n",
    "                # Get the table element\n",
    "                table = driver.find_element(By.ID, \"news-table\")\n",
    "                # Get all the rows in the table\n",
    "                rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                # Initialize an empty list to store the data\n",
    "                data = {'datetime':[], 'headlines':[], 'links':[]}\n",
    "                # Loop through the rows and extract the data\n",
    "                for row in rows:    \n",
    "                    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                    data['datetime'].append(cells[0].text)\n",
    "                    data['headlines'].append(cells[1].text)\n",
    "                    data['links'].append(cells[1].find_element(By.TAG_NAME, \"a\").get_attribute(\"href\"))\n",
    "\n",
    "                # Create dataframe\n",
    "                df = pd.DataFrame(data)\n",
    "                #removing the sources after '\\n'\n",
    "                df['headlines'] = df['headlines'].apply(lambda x: x.split('\\n')[0].lower())\n",
    "                #split the datetime format and create new columns for date and time\n",
    "                for i,date_time in enumerate(df['datetime']):\n",
    "                    if len(date_time.split(' ')) == 1:\n",
    "                        df.loc[i, 'date'] = df.loc[i-1, 'date']\n",
    "                    elif len(date_time.split(' ')) == 2:\n",
    "                        df.loc[i, 'date'] =  date_time.split(' ')[0]\n",
    "                    df.loc[i, 'time'] = date_time.split(' ')[-1]\n",
    "\n",
    "                #create column for ticker\n",
    "                df['ticker'] = t\n",
    "                #convert to date and time format\n",
    "                df['date'] = df['date'].apply(lambda x: pd.to_datetime(x, format='%b-%d-%y'))\n",
    "                df['time'] = df['time'].apply(lambda x: pd.to_datetime(x, format='%I:%M%p').time())\n",
    "                #drop the original datetime column\n",
    "                df = df.drop('datetime', axis=1)\n",
    "                # Append the current dataframe to the concatenated dataframe\n",
    "                df_concat = pd.concat([df_concat, df], ignore_index=True)\n",
    "\n",
    "            else:\n",
    "                print(f\"Title does not contain {t}\")\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for title element\")\n",
    "\n",
    "        finally:\n",
    "            driver.quit()\n",
    "            \n",
    "    # Filter the dataframe based on the start and end dates\n",
    "    if start_date is not None:\n",
    "        df_concat = df_concat.query('date >= @start_date')\n",
    "    if end_date is not None:\n",
    "        df_concat = df_concat.query('date <= @end_date')\n",
    "            \n",
    "    # Return the concatenated dataframe\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96a039",
   "metadata": {},
   "source": [
    "##### Function to scrap latest 100 stock info from Yahoo Finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8eafd83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_stock_info(ticker):\n",
    "    \n",
    "    # create an empty dataframe\n",
    "    df_concat = pd.DataFrame()\n",
    "    \n",
    "    for t in ticker:\n",
    "        path = r\"C:\\ChromeDriver\\chromedriver.exe\"\n",
    "        service = Service(path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get('https://sg.finance.yahoo.com/')\n",
    "\n",
    "        # Search engine\n",
    "        search = driver.find_element(By.ID, 'yfin-usr-qry')\n",
    "        search.send_keys(t)\n",
    "        search.send_keys(Keys.RETURN) \n",
    "\n",
    "        try:\n",
    "            # Wait for the historical button to become clickable\n",
    "            wait = WebDriverWait(driver, 10)        \n",
    "            historical_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//span[text()=\"Historical data\"]')))\n",
    "            historical_button.click()\n",
    "\n",
    "            # Wait up to 10 seconds for the class element to be present\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//table[@class=\"W(100%) M(0)\"]'))) \n",
    "            table = driver.find_element(By.XPATH, '//table[@class=\"W(100%) M(0)\"]')\n",
    "            rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "            stock_info = {'date':[], 'ticker':[], 'open':[], 'high':[], 'low':[], 'close':[], 'adj_close':[], 'volume':[]}\n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                # Only append the appropriate cell for each key in stock_info\n",
    "                if len(cells) == 7:\n",
    "                    stock_info['ticker'].append(t)\n",
    "                    stock_info['date'].append(cells[0].text)\n",
    "                    stock_info['open'].append(cells[1].text)\n",
    "                    stock_info['high'].append(cells[2].text)\n",
    "                    stock_info['low'].append(cells[3].text)\n",
    "                    stock_info['close'].append(cells[4].text)\n",
    "                    stock_info['adj_close'].append(cells[5].text)\n",
    "                    stock_info['volume'].append(cells[6].text) \n",
    "            #create dataframe\n",
    "            df = pd.DataFrame(stock_info)\n",
    "            #convert to date format\n",
    "            df['date'] = df['date'].apply(lambda x: pd.to_datetime(x, format='%d %b %Y'))\n",
    "            #convert price to int\n",
    "            df[['open', 'high', 'low', 'close', 'adj_close']] = df[['open', 'high', 'low', 'close', 'adj_close']].astype(float)\n",
    "            # remove commas from the 'volume' column and convert it to integer type\n",
    "            df['volume'] = df['volume'].str.replace(',', '').astype(int)\n",
    "            # Append the current dataframe to the concatenated dataframe\n",
    "            df_concat = pd.concat([df_concat, df], ignore_index=True)\n",
    "            \n",
    "        except TimeoutException:\n",
    "                print(\"Timed out\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14455a0a",
   "metadata": {},
   "source": [
    "# Function for Sentiment Prediction (Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f15432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_prediction_text(text):\n",
    "    \n",
    "    #tokernization\n",
    "    tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "    token = tokenizer.batch_encode_plus([text],\n",
    "                                        padding='max_length',\n",
    "                                        max_length=80,              #following best model finbert 1.1\n",
    "                                        add_special_tokens=True,\n",
    "                                        truncation=True,             \n",
    "                                        return_attention_mask=True,  \n",
    "                                        return_tensors='tf')\n",
    "    input_ids = token['input_ids']                                   \n",
    "    token_type_ids = token['token_type_ids']\n",
    "    attention_masks = token['attention_mask']\n",
    "\n",
    "    # create dataset from new input data\n",
    "    new_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_masks\": attention_masks,\n",
    "        \"token_type_ids\": token_type_ids\n",
    "    })).batch(1)                                                    #training model uses 32 because of large array of tensors\n",
    "\n",
    "    # Load the BERT model with the custom object scope\n",
    "    model = tf.keras.models.load_model('model_checkpoint/finbert1.1.h5',\n",
    "                                       custom_objects={\"TFBertForSequenceClassification\": transformers.TFBertForSequenceClassification})\n",
    "\n",
    "    # make predictions on the new data\n",
    "    predictions = model.predict(new_dataset)\n",
    "\n",
    "    # The predictions will be a probability distribution over the classes, use argmax to find the highest prob\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Define a mapping from class labels to sentiment labels\n",
    "    class_to_sentiment = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    # Map the predicted class labels to sentiment labels using numpy\n",
    "    predicted_sentiments = np.vectorize(class_to_sentiment.get)(predicted_classes)\n",
    "\n",
    "    return predicted_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8158e48",
   "metadata": {},
   "source": [
    "# Function for Sentiment Prediction (Dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163a0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_prediction_df(df):\n",
    "    \n",
    "    #list of text\n",
    "    headline_list = []\n",
    "    for headline in df['headlines']:\n",
    "        headline_list.append(headline)\n",
    "                          \n",
    "    #tokernization\n",
    "    tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "    token = tokenizer.batch_encode_plus(headline_list,\n",
    "                                        padding='max_length',\n",
    "                                        max_length=80,              #following best model finbert1.1\n",
    "                                        add_special_tokens=True,\n",
    "                                        truncation=True,             \n",
    "                                        return_attention_mask=True,  \n",
    "                                        return_tensors='tf')\n",
    "    input_ids = token['input_ids']                                   \n",
    "    token_type_ids = token['token_type_ids']\n",
    "    attention_masks = token['attention_mask']\n",
    "\n",
    "    # create dataset from new input data\n",
    "    new_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_masks\": attention_masks,\n",
    "        \"token_type_ids\": token_type_ids\n",
    "    })).batch(32)                                                    \n",
    "    \n",
    "    # Load the BERT model with the custom object scope\n",
    "    model = tf.keras.models.load_model('model_checkpoint/finbert1.1.h5',\n",
    "                                       custom_objects={\"TFBertForSequenceClassification\": transformers.TFBertForSequenceClassification})\n",
    "\n",
    "    # make predictions on the new data\n",
    "    predictions = model.predict(new_dataset)\n",
    "\n",
    "    # The predictions will be a probability distribution over the classes, use argmax to find the highest prob\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Define a mapping from class labels to sentiment labels\n",
    "    class_to_sentiment = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    # Map the predicted class labels to sentiment labels using numpy\n",
    "    predicted_sentiments = np.vectorize(class_to_sentiment.get)(predicted_classes)\n",
    "\n",
    "    # Add the predicted sentiments as a new column to the input DataFrame\n",
    "    df['predicted_sentiment'] = predicted_sentiments\n",
    "    \n",
    "    #function to assign int score\n",
    "    def score(sentiment):\n",
    "        if sentiment == 'neutral':\n",
    "            return 0\n",
    "        elif sentiment == 'positive':\n",
    "            return 1\n",
    "        elif sentiment == 'negative':\n",
    "            return -1\n",
    "    \n",
    "    #create a column for sentiment in int (0,1,2)\n",
    "    df['score'] = df['predicted_sentiment'].apply(lambda x: score(x))\n",
    "    \n",
    "    # Return the modified DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e6b9e",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4019ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "faang = get_finviz_headlines(['AAPL', 'META', 'AMZN', 'NFLX', 'GOOG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7d3b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 29s 2s/step\n"
     ]
    }
   ],
   "source": [
    "faang = sentiment_prediction_df(faang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421e8f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>links</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>ticker</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>britain's warren buffett recently bought stock...</td>\n",
       "      <td>https://finance.yahoo.com/m/e167c08c-1cf2-3d6a...</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple mixc shenzhen opens friday, april 28, in...</td>\n",
       "      <td>https://finance.yahoo.com/news/apple-mixc-shen...</td>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple developing ai-powered health coaching se...</td>\n",
       "      <td>https://finance.yahoo.com/news/apple-developin...</td>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>17:39:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tiktok, twitter, meta face countdown to comply...</td>\n",
       "      <td>https://finance.yahoo.com/m/98869e1e-b8b1-39bd...</td>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>13:56:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple wins another court ruling. its app store...</td>\n",
       "      <td>https://finance.yahoo.com/m/c34d5d8c-ccc1-3754...</td>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>12:59:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>elon musk, other ai bigwigs call for pause in ...</td>\n",
       "      <td>https://finance.yahoo.com/m/5951e4eb-0696-3054...</td>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>08:33:00</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>youtube looking into gandhis claim political v...</td>\n",
       "      <td>https://finance.yahoo.com/m/13dc1b3d-cbbd-3f8a...</td>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>06:58:00</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>three of the biggest trends in retail today</td>\n",
       "      <td>https://finance.yahoo.com/video/three-biggest-...</td>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>21:31:00</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>nfl, redbird team up to distribute sunday tick...</td>\n",
       "      <td>https://finance.yahoo.com/m/e764fdf6-f52b-3b99...</td>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>18:24:00</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>alibaba stock continues to climb after announc...</td>\n",
       "      <td>https://finance.yahoo.com/video/alibaba-stock-...</td>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>15:52:00</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headlines   \n",
       "0    britain's warren buffett recently bought stock...  \\\n",
       "1    apple mixc shenzhen opens friday, april 28, in...   \n",
       "2    apple developing ai-powered health coaching se...   \n",
       "3    tiktok, twitter, meta face countdown to comply...   \n",
       "4    apple wins another court ruling. its app store...   \n",
       "..                                                 ...   \n",
       "495  elon musk, other ai bigwigs call for pause in ...   \n",
       "496  youtube looking into gandhis claim political v...   \n",
       "497        three of the biggest trends in retail today   \n",
       "498  nfl, redbird team up to distribute sunday tick...   \n",
       "499  alibaba stock continues to climb after announc...   \n",
       "\n",
       "                                                 links       date      time   \n",
       "0    https://finance.yahoo.com/m/e167c08c-1cf2-3d6a... 2023-04-26  08:00:00  \\\n",
       "1    https://finance.yahoo.com/news/apple-mixc-shen... 2023-04-25  23:30:00   \n",
       "2    https://finance.yahoo.com/news/apple-developin... 2023-04-25  17:39:00   \n",
       "3    https://finance.yahoo.com/m/98869e1e-b8b1-39bd... 2023-04-25  13:56:00   \n",
       "4    https://finance.yahoo.com/m/c34d5d8c-ccc1-3754... 2023-04-25  12:59:00   \n",
       "..                                                 ...        ...       ...   \n",
       "495  https://finance.yahoo.com/m/5951e4eb-0696-3054... 2023-03-29  08:33:00   \n",
       "496  https://finance.yahoo.com/m/13dc1b3d-cbbd-3f8a... 2023-03-29  06:58:00   \n",
       "497  https://finance.yahoo.com/video/three-biggest-... 2023-03-28  21:31:00   \n",
       "498  https://finance.yahoo.com/m/e764fdf6-f52b-3b99... 2023-03-28  18:24:00   \n",
       "499  https://finance.yahoo.com/video/alibaba-stock-... 2023-03-28  15:52:00   \n",
       "\n",
       "    ticker predicted_sentiment  score  \n",
       "0     AAPL             neutral      0  \n",
       "1     AAPL             neutral      0  \n",
       "2     AAPL             neutral      0  \n",
       "3     AAPL             neutral      0  \n",
       "4     AAPL             neutral      0  \n",
       "..     ...                 ...    ...  \n",
       "495   GOOG             neutral      0  \n",
       "496   GOOG            negative     -1  \n",
       "497   GOOG             neutral      0  \n",
       "498   GOOG             neutral      0  \n",
       "499   GOOG            positive      1  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f5b8784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alibaba stock continues to climb after announcement of six-way business split'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faang['headlines'][499]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5edcd3",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9c5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65d334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5de91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d817b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pls ignore the rest..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faang_stock_info = get_stock_info(['AAPL', 'META', 'AMZN', 'NFLX', 'GOOG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faang_stock_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0f2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apple = faang[faang['ticker'] == 'AAPL']\n",
    "# apple = apple.groupby('date')[['score']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fff9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(f, apple, on='date', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c752559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = faang_stock_info[faang_stock_info['ticker'] == 'AAPL'][['date', 'close', 'volume']].set_index('date')\n",
    "# f = f.pct_change().dropna(axis=0)\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07735243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faang_stock_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd29d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [15, 5]\n",
    "# mean = faang.groupby(['ticker', 'date'])['score'].mean().unstack().transpose()\n",
    "# mean.plot(kind='bar')\n",
    "# plt.grid(alpha=0.5)\n",
    "# mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
